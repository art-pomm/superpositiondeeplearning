<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
		font-family: Arial, sans-serif;
		line-height: 2;
		font-size: 20px;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: left; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}
	.main-content-block {
		width: 70%; /* Change this percentage as needed */
    max-width: 1100px; /* Optional: Maximum width */
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			padding: 5px;
	}
	.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			font-size: 14px;
			width: 25%; /* Change this percentage as needed */
			max-width: 256px; /* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;  /* Optional: Adds padding inside the caption */
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}
	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}
	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}

</style>

	  <title>Expanding the Toy Model Framework</title>
      <meta property="og:title" content="Expanding the Toy Model Framework" />
			<meta charset="UTF-8">
  </head>

  <body>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */">Foundations of Interpretability: Expanding the Toy Model Framework</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:17px"><a href="blog_template/your_website">Joseph Atie</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="blog_template/your_partner's_website">Abhilash Rao</a></span>
										</td>
										<td align=left>
											<span style="font-size:17px"><a href="blog_template/your_partner's_website">Arthur Pommersheim</a></span>
									</td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

		<div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <!-- table of contents here -->
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <a href="#definitions">Terminology and Definitions</a><br><br>
              <a href="#currentwork">Current Work</a><br><br>
			  <a href="#Experiment 1">Experiment 1</a><br><br>
			  <a href="#Experiment 2">Experiment 2</a><br><br>
			  <a href="#Experiment 3">Experiment 3</a><br><br>
              <a href="#implications_and_limitations">Implications and limitations</a><br><br>
			  <a href="#citations">Citations</a><br><br>



          </div>
				</div>
		    <div class="main-content-block">
            <!--You can embed an image like this:-->
            
		    </div>
		</div>

    <div class="content-margin-container" id="intro">
			<div class="margin-left-block">
			</div>
		    <div class="main-content-block">
						<h2>Introduction</h2>
						<p>
						In the ever-expanding domain of deep learning, the ability of neural networks to process and represent high-dimensional data efficiently 
						remains one of their most remarkable yet least understood capabilities. A key aspect of this efficiency lies in dimensionality reduction, 
						where networks compress vast amounts of information into lower-dimensional embeddings while retaining meaningful features. This process is 
						central to the power of deep learning, enabling models to generalize, reduce noise, and efficiently handle the curse of dimensionality.
						</p>
						<p>
						However, despite its importance, many papers have compared dimensionality reduction to a "black box", making neural network less interpretable. <sup><a href="#ref_1">[1]</a></sup>
						As students of deep learning, we were inspired by various sources exploring the embedding space and visualizing neural networks to better understand these complex systems. <sup><a href="#ref_2">[2]</a><a href="#ref_3">[3]</a></sup><sup><a href="#ref_4">[4]</a></sup>  However, Principal Component Analysis, T-Distributed Stochastic Neighbor Embedding, and various graph-based representations left us hungry for more intuition. 
						
						<div class="image-container">
							<img src="images/blackbox.png" style="width: 50%;">
						</div>	
						
						Wouldn't it have been nicer if each neuron in the neural network represented only one feature? For example, in the context of CNNs, it would be more 
						intuitive if one neuron were solely responsible for detecting curvature, while another solely focused on detecting color. Unfortunately the reality 
						is different. When a network creates a compact embedding, each feature meets one of three fates: </p>
			    
						<p>
						(1) The feature is exclusively encoded by a single dedicated monosementic neuron. <br>
						(2) The feature is jointly encoded by a polysementic neuron alongside another feature (superposition). <br>
						(3) The feature is ignored by the model.
						</p>
						<p>
						It is specifically the phenomenon observed in (2)—polysemantic encoding—that makes neural networks less interpretable. To build intuition around 
						this, recent work such as <i>Toy Models of Superposition</i><sup><a href="#ref_5">[5]</a></sup> has begun to uncover the mechanism of superposition. These studies use ReLU networks to investigate 
						how a neural network decides to encode an input feature based on factors like feature importance and sparsity.
						</p>
						<p>
						<b>The gap this project aims to fill</b><br>
						While these studies provide an important first step toward building intuition about encoding mechanisms, they leave critical practical questions unanswered. For instance, how do choices in network 
						architecture, such as activation functions, impact the emergence and stability of superposition? How does the model decide which important features to prioritize over less important ones in the limited embedding space? How do different weight initialization methods influence Toy model's ability to represent features? <br><br>
						Our project aims to address these questions by systematically exploring the dynamics of superposition within simplified toy models. Using these 
						interpretable frameworks, we will experiment with altering key factors, such as activation functions, optimizers, and network configurations, to 
						observe their effects on feature representation. These experiments are scientifically significant because they probe fundamental aspects of 
						representation learning that are critical to understanding how neural networks encode and compress information. While dimensionality reduction 
						has been extensively studied in larger models, the direct effects of architectural and training choices on superposition remain largely unexplored, 
						particularly in controlled, interpretable environments like toy models.
						</p>
						<p>
						<b>List of experiments</b><br>
						<ul>
							<li><b>Experiment 1:</b> Altering the activation function </li>
							<li><b>Experiment 2:</b> Altering features importance </li>
							<li><b>Experiment 3:</b> Effect of initialization methods </li>
						</ul>	
						</p>
						<p>
						<b>Importance of this work</b><br>
						Our project advances theoretical understanding and provides actionable insights into designing more efficient and interpretable neural 
						networks, making it a valuable addition to the growing body of research in this area.
						</p>

		    </div>
		</div>

		<div class="content-margin-container" id="definitions">
			<div class="margin-left-block">
			</div>
		    <div class="main-content-block">
					<h2>Terminology and Definitions</h2>
					Before presenting our experimental results, we will define essential terminology to establish a common conceptual framework and ensure clarity throughout this study.
						<p><b>Features</b><br>
							The interpretation of what constitutes a feature varies across studies and applications. 
							In this study, we define features as input qualities that help differentiate one input from another. 
							Features can sometimes be directly observable, such as curvature or color in an image. 
							At other times, they may be conceptual and interpretable by humans, as demonstrated by the famous work 
							of Mikolov et al. <sup><a href="#ref_6">[6]</a></sup>, which showed that word embeddings can have directions corresponding to semantic 
							properties. This allows for operations like embedding arithmetic, exemplified by:
						</p>
						<blockquote>
							V("king") − V("man") + V("woman") = V("queen")
						</blockquote>
						<p>
							However, some features defy human understanding. Should this surprise us? It did—until we were inspired by the insights of Richard Hamming and Brett Victor in
							<i>The Unreasonable Effectiveness of Mathematics</i> and <i>Media for Thinking the Unthinkable</i>. Their perspectives help explain why certain concepts remain 
							elusive and how new frameworks can expand our comprehension:
						</p>
						<blockquote>
							"Just as there are odors that dogs can smell and we cannot, as well as sounds that dogs can hear and 
							we cannot, so too there are wavelengths of light we cannot see and flavors we cannot taste.
							Why then, given our brains wired the way they are, does the remark 'Perhaps there are thoughts we 
							cannot think,' surprise you?"
							<cite>- Richard Hamming, <i>The Unreasonable Effectiveness of Mathematics</i></cite>
						</blockquote>
						<blockquote>
							"These sounds that we can't hear, this light that we can't see, how do we even know about these things 
							in the first place? Well, we built tools. We built tools that adapt these things that are outside of 
							our senses, to our human bodies, our human senses. 
							We can't hear ultrasonic sound, but you hook a microphone up to an oscilloscope and there it is. 
							You're seeing that sound with your plain old monkey eyes. We can't see cells and we can't see galaxies, 
							but we build microscopes and telescopes and these tools adapt the world to our human bodies, to our 
							human senses. <br><br>
							When Hamming says there could be unthinkable thoughts, we have to take that as 'Yes, but we build 
							tools [neural networks] that adapt these unthinkable thoughts to the way that our minds work and 
							allow us to think these thoughts that were previously unthinkable."
							<cite>- Brett Victor, <i>Media for Thinking the Unthinkable</i></cite>
						</blockquote>

					</p>

					<p><b>Superposition</b><br>
					Imagine you're at a packed movie theater, but instead of seats, you have a limited number of beanbags. Each beanbag 
					can be shared by multiple people, as long as they squeeze in carefully and don’t completely overlap. Somehow, everyone 
					manages to sit comfortably, even though the number of beanbags seems too small for the crowd. This is superposition in action. For a quick visual primer we highly recommend viewing <a href="#ref_7">[7]</a>.</p>
					<p>
					In neural network, superposition is a clever strategy allowing neural networks "to represent more features than they have neurons". 
					They exploit a property of high-dimensional spaces to simulate a model with many more neurons. 
					</p>

					<p><b>Sparsity</b><br>
					Feature Sparsity is a concept rooted in the observation that, in the natural world, many features are sparse—they occur only occasionally. For example, most parts of an image don’t 
					contain specific objects like a horizontal edge or a dog head, and most tokens in language don’t refer to rare concepts like "Martin Luther King" or "music." This idea builds on 
					classical work in vision and natural image statistics, where sparsity helps models focus only on the most relevant features at any given time. <br>
					In the toy model, feature sparsity is modeled using the probability <em>S</em>, which determines how often a feature in the 
					input vector is inactive (set to 0). Specifically:
					</p>
						<ul>
							<li>For a given dimension <em>x<sub>i</sub></em>, the feature is set to 0 with probability <em>S<sub>i</sub></em>, meaning it is inactive most of the time if <em>S<sub>i</sub></em> is high.</li>
							<li>Otherwise, with probability <em>1 - S<sub>i</sub></em>, the feature is active and takes a value uniformly distributed between 0 and 1.</li>
						</ul>
						<p>
							In practice, the toy model we're analyzing in this blog post assumes all features have the same sparsity <em>S</em>, meaning that all features share the same probability of being inactive. This sparse 
							distribution reflects real-world data structures, where only a subset of features is typically relevant at any given time.
						</p>
					<p>
						To gain a deeper understanding of the concept of sparsity, we modified the toy model to generate three distinct batches with varying levels of feature sparsity:
					</p>
						<ul>
							<li><strong>S = 0:</strong> Features are never set to 0 (all features are active).</li>
							<li><strong>S = 0.5:</strong> Features are set to 0 with a probability of 0.5, resulting in approximately half of the features being inactive.</li>
							<li><strong>S = 0.9:</strong> Features are set to 0 with a probability of 0.9, making most features inactive.</li>
						</ul>
					<p>
						The figures below illustrate the outputs for these different levels of sparsity, providing insights into how feature activation changes as the sparsity increases.
					</p>					
					<div class="image-container">
						<div class="image-caption"><strong>Figure 1: Output with S = 0 (All features active)</strong></div>
						<img src="images/sparsity0.png" alt="Sparsity with S=0">
					</div>
					<div class="image-container">
						<div class="image-caption"><strong>Figure 2: Output with S = 0.5 (Approximately half features active)</strong></div>
						<img src="images/sparsity05.png" alt="Sparsity with S=0.5">
					</div>
					<div class="image-container">
						<div class="image-caption"><strong>Figure 3: Output with S = 0.9 (Most features inactive)</strong></div>
						<img src="images/sparsity09.png" alt="Sparsity with S=0.9">
					</div>

					<p><b>Feature Importance</b><br>
					Imagine packing a suitcase for a trip where certain items, like your passport or medicine, are far more critical than others, such as an extra pair of shoes. 
					Naturally, you would prioritize these essential items to avoid serious consequences later. <br>
					Similarly, in the toy model, not all features are treated equally—some are far more important than others. In the toy model, feature importance assigns weights 
					<em>I<sub>i</sub></em> to each feature, reflecting its significance in the overall reconstruction. When the model makes predictions, the reconstruction error for each feature is scaled 
					by its importance. This means that disregarding or poorly reconstructing a highly important feature incurs a much higher cost than for a less important one. 
					The model is incentivized to focus its representational capacity on preserving more important features, ensuring that key patterns in the data are retained while 
					less critical ones can be approximated or ignored.
					</p>
					</p>	
		    </div>
		</div>

		<div class="content-margin-container" id="currentwork">
			<div class="margin-left-block">
			</div>
		    <div class="main-content-block">
					<h2>Background and Model to Demonstrate Superposition</h2>
					<p>
						The experiment setup in this section creates a simplified environment similar to the one used in <i>Toy Models for Superposition</i><sup><a href="#ref_5">[5]</a></sup> to analyze how neural networks encode and reconstruct information when their capacity is limited.
					</p>
					<ul>
						<li>
							<strong>Input Features:</strong> The input consists of vectors (<em>x</em>) of fixed size <em>n</em>, with <em>n</em> features, where each feature is represented by a scalar value.
						</li>
						<li>
							<strong>Feature Sparsity:</strong> Features are not always active. A sparsity parameter (<em>S</em>) determines the probability that a feature is set to 0.
						</li>
						<li>
							<strong>Feature Importance Weights:</strong> Each feature has an associated importance value (<em>I</em>), influencing its contribution to the loss function. Higher importance features incur a greater penalty if not accurately reconstructed.
						</li>
						<li>
							<strong>Encoding and Decoding:</strong> The model uses a limited number of hidden units <em>m</em> (less than the total number of features, <em>m < n</em>) to encode the input, creating a bottleneck. The decoder then attempts to reconstruct the input features from this compressed representation.
						</li>
						<li>
							<strong>Reconstruction Error:</strong> The error for each feature is computed as the squared difference between the original and reconstructed values. This error is weighted by the feature's importance, prioritizing accurate reconstruction of important features.
						</li>
						<li>
							<strong>Optimization:</strong> The model is trained to minimize the total weighted reconstruction error, encouraging efficient use of its limited capacity while focusing on high-importance features.
						</li>
					</ul>
					<div class="image-container">
						<div class="image-caption"><strong>Figure 4: Network Architecture</strong></div>
						<img src="images/network_architecture.png" alt="Network Architecture">
					</div>
					<p>The toy model developed by OpenAI<sup><a href="#ref_5">[5]</a></sup> employs the ReLU activation function to investigate how neural networks decide to encode each feature. This simplified approach helps uncover the fundamental mechanisms of feature representation. As we progress through this blog, we'll delve into various 
						activation functions and examine how they influence the encoding strategies of neural networks, highlighting their impact on our findings.</p>
					<div>
						<div class="title">Loss Function: </div>
						<div class="equation">
							\( L = \sum_{x} \sum_{i} I_i (x_i - x_i')^2 \)
						</div>
					</div>
					<br>
					<div class="title">ReLU Output Model Equations:</div>
						<div class="equation">
							\( h = Wx \)
						</div>
						<div class="equation">
							\( x' = \text{ReLU}(W^T h + b) \)
						</div>
						<div class="equation">
							\( x' = \text{ReLU}(W^T Wx + b) \)
						</div>
		    		</div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h2>Basic Results</h2>
            
						
		    </div>

		</div>

		<div class="content-margin-container" id="Experiment 1">
				<div class="margin-left-block">
				</div>
			<div class="main-content-block">
			<h2>Experiment 1: Altering Activation Functions</h2>
				<p>
					This experiment investigates how different activation functions influence the emergence of superposition and the allocation of representational capacity in neural networks. Activation functions determine how input signals are transformed as they propagate through a network, playing a key role in encoding and decoding features. By altering the activation function, we aim to understand how the network's ability to separate, combine, or ignore features changes under varying conditions. Mid-way through the project, we came across existing research [7] directly related to this topic which further validated our hypothesis and initial research motivation. Moving forward, however, we decided to focus majority of our efforts on experiment 2 and 3 while limiting experiment 1 to just the three most popular activation functions i.e. ReLU, Sigmoid, Tanh. 
				</p>
				<p>
					Model Configuration:
					<ul class="config-list">
						<li>Total number of features \(n\): 20</li>
						<li>Embedding size \(m\): 5</li>
						<li>Features Importance: Uniform across features</li>
						<li>Activation function: ReLU, Sigmoid, Tanh</li>
						<li>Input vector sparsity \(S\): 0</li>
					</ul>
				</p>
				<p>
					<h1>The following images are the WtW matrices and the feature-neuron mappings.They help us validate that our results are in-line with those observed in [] and []</h>
					
				</p>
				<p>
					<h1>ReLU:</h1> 
				</p>
				<p>
					<div class="image-container">
						<div class="image-caption"><strong></strong></div>
						<img src="images/ReLU Activation Function Experiment Plot.png" alt="Image">
					</div>
					
					The ReLU model focuses on the most significant features in the low sparsity regime (generally resulting in monosemanticity), while relying on superposition in the high sparsity regime (polysemanticity). With weaker signals for the most important features in the high sparsity regime, the model encodes multiple features in each neuron activation to minimize error of the sparse signals. Notably, the ReLU model uses antipodal pairs in the mapping of features to encode multiple features to single neurons. This can be seen as a light-colored diagonal entry within WtW and a corresponding dark-colored off-diagonal entry within the same column. This antipodal mapping of features is a method that the model uses to compress more than one feature to one neuron.
				</p>	
					<h2>Sigmoid:</h2>
				<p>
					<div class="image-container">
						<div class="image-caption"><strong></strong></div>
						<img src="images/Sigmoid Activation Function Experiment Plot.png" alt="Image">
					</div>

					The Sigmoid model exhibits superposition in all neurons as soon as the sparsity is non-zero, as can be seen from the “speckling” of non-zero off-diagonal terms in WtW.This is a difference from the ReLU/GeLU/SiLU models, for which the superposition “leaks” into the least significant encoded features at low, non-zero sparsities and eventually affects all features at higher sparsities. This low-sparsity superposition may occur because the Sigmoid function strictly maps to (0,1)(0,1), with increasingly large pre-activation inputs necessary to map to values close to 0 and 1. As such, the model may be “speckling” the off-diagonal values in an attempt to “reach” these inputs which are close to 0 and 1.
				</p>	
					<h3>Tanh:</h3>

					<div class="image-container">
						<div class="image-caption"><strong></strong></div>
						<img src="images/Tanh Activation Function Experiment Plot.png" alt="Image">
					</div>

					With the Tanh activation function, the models prioritize the most important features regardless of sparsity. This behavior is possibly attributed to the range that the Tanh function maps to (−1,1)(−1,1), while the target range of input values in this experiment are [0,1][0,1]. This behavior is similar to that of a linear model (i.e., no activation function) which exhibits no capability to use superposition, but the phase diagram reveals subtle differences from the linear model results.
				</p>
			</div>
					
		</div>

		<div class="content-margin-container" id ="Experiment 2">
				<div class="margin-left-block">
				</div>
			<div class="main-content-block">
			<h2>Experiment 2: Altering Feature Importance</h2>
				<p>
					A simple way to understand which features the network is representing is by examining the matrix \( W^T W \), which has dimensions \( n \times n \), 
					where \( n \) is the number of features in the input vector. This matrix encodes how the network uses its embedding space to represent the input features.
					The diagonal elements, located at \( (i, i) \), indicate whether feature \( i \) is being represented by the network. 
				</p>
				<p>
					In this section, we aim to isolate the effect of feature importance on a neural network's decision to encode features. 
					To achieve this, we set the feature sparsity to \( S = 0 \) without modifying it, meaning the input vector is very dense. With this setup, the neural network determines which features to encode or 
					ignore based solely on their importance values. The activation function used in this experiment is ReLU. 
				</p>
				<h1>Test Case 1: Features are ordered from more important to less important</h1>
				<p>
				In this test case, the input features are ordered by importance, starting with the most important and decreasing to the least important. 
				The importance values follow an exponential decay, with the most critical features receiving significantly higher weights than the less important ones. The input vector size is 20 and the embedding size is 5.
				</p>
				<p>
					Model Configuration:
					<ul class="config-list">
						<li>Total number of features \(n\): 20</li>
						<li>Embedding size \(m\): 5</li>
						<li>Features Importance: Follows exponential decay</li>
						<li>Activation function: ReLU</li>
						<li>Input vector sparsity \(S\): 0</li>
					</ul>
				</p>
				<p>
					Results:
				</p>
				<p>
					In Figure 5, the left plot represents the L2 norms of the weight vectors corresponding to each feature. Each bar indicates the magnitude of a feature's contribution to the embedding space, 
					with taller darker blue bars reflecting stronger representation. In this case, the plot shows that the network represents exclusively the first five features, which are the five features with the highest importance as they are ordered by descending importance. 
					These features are fully represented. The absence of bars for the remaining features indicates that these features 
					have been completely disregarded by the network due to capacity constraints in the embedding space. The model hasn't superposed any features together in the embedding space.
				</p>
				<p>
					The right plot is a heatmap showing the pairwise interference between features, computed as the dot 
					product of their weight vectors \(W^T W\). Each cell in the heatmap, outside the diagonal (i, i), corresponds to the interaction between two features, with 
					brighter colors (red) indicating stronger interference. In this case, the heatmap reveals that the network's 
					embedding is highly orthogonal, as all off-diagonal elements are neutral (gray), indicating no overlap between 
					features. The strong diagonal elements confirm that the first five features are self-represented, with little to no 
					interference between different features.
				</p>
				<p>
					As our standard intuitions would expect, the model consistently selects the top-\( m \) most important features for the embedding. In our specific case, the model 
					selected the first 5 features—those with the highest importance—and ignored all remaining features. The behavior of the bias vector \( b \) is also noteworthy. 
					For the features selected to pass through the embedding, the corresponding entries in \( b \) are set to zero. For all other features, \( b \) contains the expected 
					value of the feature, ensuring that disregarded features do not contribute to the model's output.
					<div class="image-container">
						<div class="image-caption">Figure 5: Test Case 1 Results</div>
						<img src="images/features_decay_importance.png" alt="Features by Importance">
					</div>
				</p>
				<h1>Test Case 2: All features are assigned equal importance</h1>
				<p>
				In this test case, the input features are assigned equal importance. The purpose of this test case is to evaluate how the neural network would select which features to represent 
				when all features are equally important and when there isn't enough neurons to represent all features.
				</p>
				<p>
					Model Configuration:
					<ul class="config-list">
						<li>Total number of features \(n\): 20</li>
						<li>Embedding size \(m\): 5</li>
						<li>Features Importance: Uniform across features</li>
						<li>Activation function: ReLU</li>
						<li>Input vector sparsity \(S\): 0</li>
					</ul>
				</p>
				<p>
					Results:
				</p>
				<p>
					When all features are assigned equal importance, the neural network distributes its representational capacity more evenly across the input features. This leads to a noticeable 
					increase in feature interference, as shown in the heatmap, where overlapping representations are more prevalent. Interestingly, despite equal importance, certain features still 
					exhibit slightly higher norms, suggesting that the network inherently favors specific features, possibly due to subtle biases in the initialization or training dynamics. This 
					result highlights the critical role of prioritization in reducing interference and efficiently utilizing limited embedding space. When feature importance is unequal, the network 
					can focus on the most critical features, minimizing overlap and improving separability, whereas equal importance forces the network to rely more heavily on superposition, potentially 
					compromising representational efficiency.
					<div class="image-container">
						<div class="image-caption">Figure 6: Test Case 2 Results</div>
						<img src="images/features_uniform_importance.png" alt="Features by Importance">
					</div>
				</p>
				<h1>Test Case 3: Clustered Importance</h1>
				<p>
					In this clustered importance test case, we assign distinct importance levels to groups of input features, creating high- and low-priority clusters.
				</p>
				<p>
					Model Configuration:
					<ul class="config-list">
						<li>Total number of features \(n\): 20</li>
						<li>Embedding size \(m\): 5</li>
						<li>Features Importance: [1, 1, 1, 1, 1, 0.5, 0.5, 0.5, 0.5, 0.5, 1, 1, 1, 1, 1, 0.5, 0.5, 0.5, 0.5, 0.5]</li>
						<li>Activation function: ReLU</li>
						<li>Input vector sparsity \(S\): 0</li>
					</ul>
				</p>
				<p>
					Results:
				</p>
				<p>
					The clustered importance experiment reveals surprising behavior in how neural networks allocate their representational capacity. Despite assigning equal importance to the first and third five features, 
					the model favors the 4<sup>th</sup>, 5<sup>th</sup>, 11<sup>th</sup>, 12<sup>th</sup>, 13<sup>th</sup>, and 15<sup>th</sup>, while the other four features with high importance are deprioritized. This indicates that the network does not strictly follow the assigned importance weights but instead 
					considers other factors, such as redundancy among features or the ease of representing certain features in the reduced embedding space. The features with lower importance 
					(importance = 0.5) were not represented by the model.
					<div class="image-container">
						<div class="image-caption">Figure 7: Test Case 3 Results</div>
						<img src="images/features_cluster_importance.png" alt="Features by Importance">
					</div>
				</p>
			</div>
		</div>

	</div>

	<div class="content-margin-container" id="Experiment 3">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h2>Experiment 3: Effect of Initialization Methods</h2>
			<p>
				In this experiment, we explore how different weight initialization methods influence Toy model's ability to represent features. Initialization plays a critical role in the training dynamics of neural networks, determining the starting point for optimization and affecting how quickly and effectively the model converges. Here, we compare several initialization methods:
				<ul>
					<li><strong>Xavier Normal</strong>: Ensures the variance of the weights is proportional to the input and output dimensions.</li>
					<li><strong>Xavier Uniform</strong>: Similar to Xavier Normal but initializes weights uniformly.</li>
					<li><strong>He Normal</strong>: Optimized for ReLU activation, with higher variance for weights connected to deeper layers.</li>
					<li><strong>Uniform (-0.1, 0.1)</strong>: Assigns weights randomly between -0.1 and 0.1.</li>
					<li><strong>Normal (mean=0, std=0.05)</strong>: Initializes weights with a Gaussian distribution of mean 0 and standard deviation 0.05.</li>
				</ul>
			</p>
			<p>
				Model Configuration:
				<ul class="config-list">
					<li>Total number of features \(n\): 20</li>
					<li>Embedding size \(m\): 5</li>
					<li>Features Importance: 0.7<sup>i</sup></li>
					<li>Activation function: ReLU</li>
					<li>Input vector sparsity \(S\): [0.0, 0.7, 0.9, 0.97, 0.99, 0.997, 0.999]</li>
				</ul>
			</p>
			We aim to understand which features the model represents in its hidden layers and whether those features are independent of each other. To do this, we look at the size of each feature's vector,  \(∣∣W_i∣∣\), where a value close to 1 means the feature is fully represented, and a value of 0 means it is not. The blue/yellow color scale represents superposition, calculated as the squared dot product of feature vectors. Brighter colors (yellow) indicate greater overlap between features, while darker colors (black) show features that are more independent. As you move down each interactive plots more and more sparse inputs. 

			<p>
				The blue and red portions in the heatmap visualization represent the values of the weights, where the color indicates both the magnitude and the direction of influence. Blue represents negative values, meaning these weights suppress or reduce the corresponding features, while red represents positive values, indicating that these weights amplify or emphasize the features. The intensity of the color reflects the magnitude of the weight, with darker shades corresponding to larger values. Grey areas indicate a neutral influence, where the weight value is close to zero. This color-coded representation helps illustrate how the model assigns importance to different features and adjusts their contributions to the hidden representation.			</p>

			<!-- Xavier Normal Results -->
			<h3>Xavier Normal</h3>
			<p>
			</p>
			<iframe src="Xavier Normal.html" width="650" height="750" style="border:none;"></iframe>
	
			<!-- Xavier Uniform Results -->
			<h3>Xavier Uniform</h3>
			<p>
			</p>
			<iframe src="Xavier Uniform.html" width="650" height="750" style="border:none;"></iframe>
	
			<!-- He Normal Results -->
			<h3>He Normal</h3>
			<p>
			</p>
			<iframe src="He Normal.html" width="650" height="750" style="border:none;"></iframe>
	
			<!-- Uniform (-0.1, 0.1) Results -->
			<h3>Uniform (-0.1, 0.1)</h3>
			<p>
			</p>
			<iframe src="Uniform (-0.1, 0.1).html" width="650" height="750" style="border:none;"></iframe>
	
			<!-- Normal (mean=0, std=0.05) Results -->
			<h3>Normal (mean=0, std=0.05)</h3>
			<p>
			</p>
			<iframe src="Normal (m=0, std=0.05).html" width="650" height="750" style="border:none;"></iframe>

			<p>
				In dense regimes (\(1-S = 1.0\)), all initializations produce orthogonal feature representations, as evidenced by diagonal elements in the \(W^T W\) heatmaps and dark blue feature representations in \(∣∣W_i∣∣\). However, in sparser regimes (\(1-S \leq 0.03\)), there are differences between initializations. Notably, Uniform (-0.1, 0.1) exhibit higher variability in feature norms but align closely with other methods as sparsity increases. Our results suggests that sparsity constraints overshadow initialization differences in determining representational capacity.
			</p>
			
		</div>
</div>

		<div class="content-margin-container" id="implications_and_limitations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>Implications and Limitations</h2>
				<h1>Implications</h1>
					<p>
						The findings from this study offer practical insights that extend beyond the simplified toy model, particularly in the context of real-world applications and 
						the use of visual diagnostics for neural network interpretability. In real-world architectures such as transformers or autoencoders, where embedding size is 
						often a bottleneck, understanding how networks prioritize features can guide the design of models that efficiently allocate representational capacity. For 
						instance, tasks like text summarization or image compression often require compact embeddings while preserving key information, a challenge our results 
						address through the principles of feature prioritization and sparsity. Additionally, the study underscores the critical role of activation functions in 
						managing feature interference and representation. Activation functions such as ReLU, sigmoid, 
						and tanh influence the degree of sparsity and superposition in embeddings, suggesting that careful selection of activation functions can significantly impact 
						the quality of feature representations. The power of visual diagnostics, such as bar plots for feature norms and heatmaps for interference patterns, 
						is highlighted as a practical tool to demystify neural network behavior. These visualizations provide an intuitive understanding of how networks handle 
						overlapping representations, offering a framework for debugging and interpreting more complex models. By leveraging these insights, researchers and practitioners 
						can design models that are not only more efficient but also more interpretable, paving the way for broader adoption of explainable AI techniques in industry and 
						research.
					</p>
				<h1>Limitations</h1>
					<p>
						While our study provides valuable insights into feature prioritization and superposition in constrained embedding spaces, it is not without limitations. First, 
						the toy model operates in a controlled environment with a fixed embedding size, which simplifies real-world complexities and may not generalize 
						to hierarchical or multi-layer architectures like transformers or convolutional neural networks. Second, our analysis assumes static feature importance, whereas in 
						real-world applications, feature importance often evolves dynamically during training, influenced by data interactions and optimization processes. Additionally, while 
						the study examines final embeddings, it does not account for temporal dynamics during training, such as how representations change over epochs. Finally, biases introduced 
						by specific initialization strategies and activation functions, though partially explored, leave room for further investigation into how these choices influence superposition 
						behavior across diverse tasks. Addressing these limitations in future work could provide a deeper understanding of how neural networks balance feature prioritization and 
						representation under real-world constraints.
					</p>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_1"></a>[1] <a href="https://arxiv.org/abs/1708.08296">Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models</a>, Samek, W., Wiegand, T., & Müller, K-R., 2017<br><br>
							<a id="ref_2"></a>[2] C. Olah, <a href= "https://colah.github.io/posts/2014-10-Visualizing-MNIST/">"Visualizing MNIST: An Exploration of Dimensionality Reduction,"</a> colah's blog, Oct. 9, 2014. [Online].  [Accessed: Nov. 15, 2024]<br><br>
							<a id="ref_3"></a>[3] C. Olah, <a href= "https://colah.github.io/posts/2015-01-Visualizing-Representations/">"Visualizing Representations: Deep Learning and Human Beings,"</a> colah's blog, Jan. 16, 2015. [Online]. [Accessed: Nov. 15, 2024]<br><br>

							<a id="ref_4"></a>[4] 3Blue1Brown, <a href="https://www.youtube.com/watch?v=wjZofJX0v4M&t=748s">"Transformers (how LLMs work) explained visually,"</a> YouTube, Apr. 1, 2024. [Online].  [Accessed: Nov. 15, 2024]<br><br>

							<a id="ref_5"></a>[5] N. Elhage, T. Hume, C. Olsson, N. Schiefer, T. Henighan, S. Kravec,
							 Z. Hatfield-Dodds, R. Lasenby, D. Drain, C. Chen, R. Grosse, S. McCandlish, J. Kaplan, 
							 D. Amodei, M. Wattenberg, and C. Olah, <a href="https://transformer-circuits.pub/2022/toy_model/index.html#phase-change">"Toy Models of Superposition"</a>, Transformer Circuits, Sept. 14, 2022.
							  [Online]. [Accessed: Dec. 02, 2024].<br><br>

							  <a id="ref_6"></a>[6] T. Mikolov, W. Yih, and G. Zweig, <a href="https://aclanthology.org/N13-1090.pdf">"Linguistic regularities in continuous space word representations,"</a> Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2013, pp. 746–751. [PDF]<br><br>

							  <a id="ref_7"></a>[7] 3Blue1Brown, <a href="https://www.youtube.com/watch?v=9-Jl0dxWQs8&t=1024s">"How might LLMs store facts,"</a> YouTube, Aug. 31, 2024. [Online]. [Accessed: Nov 15, 2024]<br><br>


						</div>
		    </div>
		    <div class="margin-right-block">
            <!-- margin notes for reference block here -->
		    </div>
		</div>

	</body>

</html>
